from typing import List, Optional, Tuple
from api.src.backend.entities import EvaluationRun, EvaluationStatus, EvaluationsWithHydratedRuns, Evaluation, EvaluationsWithHydratedUsageRuns

async def get_evaluation_by_evaluation_id(evaluation_id: str) -> Evaluation: ...
async def get_evaluations_by_version_id(version_id: str) -> List[Evaluation]: ...
async def get_evaluations_for_agent_version(version_id: str, set_id: Optional[int] = None) -> list[EvaluationsWithHydratedRuns]: ...
async def get_evaluations_with_usage_for_agent_version(version_id: str, set_id: Optional[int] = None) -> list[EvaluationsWithHydratedUsageRuns]: ...
async def get_running_evaluations() -> List[Evaluation]: ...
async def get_running_evaluation_by_validator_hotkey(validator_hotkey: str) -> Optional[Evaluation]: ...
async def get_running_evaluation_by_miner_hotkey(miner_hotkey: str) -> Optional[Evaluation]: ...

async def does_validator_have_running_evaluation(validator_hotkey: str) -> bool: ...
async def does_miner_have_running_evaluations(miner_hotkey: str) -> bool: ...

async def get_queue_info(validator_hotkey: str, length: int = 10) -> List[Evaluation]: ...
async def get_agent_name_from_version_id(version_id: str) -> Optional[str]: ...
async def get_miner_hotkey_from_version_id(version_id: str) -> Optional[str]: ...
async def update_evaluation_to_error(evaluation_id: str, error_reason: str): ...
async def get_inference_success_rate(evaluation_id: str) -> Tuple[int, int, float, bool]: ...

async def reset_evaluation_to_waiting(evaluation_id: str): ...
async def update_evaluation_to_completed(evaluation_id: str): ...
async def update_evaluation_to_started(evaluation_id: str): ...
async def get_problems_for_set_and_stage(set_id: int, validation_stage: str) -> list[str]: ...
async def prune_evaluations_in_queue(threshold: float, max_set_id: int): ...
async def get_evaluation_for_version_validator_and_set(
    version_id: str,
    validator_hotkey: str,
    set_id: int
) -> Optional[str]: ...

async def create_evaluation(
    evaluation_id: str, 
    version_id: str, 
    validator_hotkey: str, 
    set_id: int, 
    screener_score: float
) -> Evaluation: ...

async def create_evaluation_runs(evaluation_runs: list[EvaluationRun]): ...

async def replace_old_agents(miner_hotkey: str) -> None: ...
async def get_progress(evaluation_id: str) -> float: ...

async def get_stuck_evaluations() -> List[Evaluation]: ...
async def get_waiting_evaluations() -> List[Evaluation]: ...
async def cancel_dangling_evaluation_runs() -> None: ...
async def evaluation_count_for_agent_and_status(version_id: str, status: EvaluationStatus): ...
async def check_for_currently_running_eval(validator_hotkey: str) -> bool: ...