"""Base class for problem suites."""

import os
import utils.logger as logger

from models.problem import Problem
from abc import ABC, abstractmethod



class ProblemSuite(ABC):
    """
    Abstract base class for a problem suite.
    Classes like PolyglotProblemSuite and SWEBenchProblemSuite inherit from this class.
    """



    def __init__(self, dataset_path: str):
        self.problems = {}
        
        self.dataset_path = dataset_path
        self.load_problems(dataset_path)



    def _add_problem(self, problem: Problem) -> None:
        """
        Adds a problem to the problem suite.
        Should only be called from within the load_problems() implementation of a derived class.
        """
        
        if problem.name in self.problems:
            logger.fatal(f"Problem {problem.name} already exists in the suite")
        
        self.problems[problem.name] = problem

    @abstractmethod
    def load_problems(self, dataset_path: str):
        """
        Loads all the problems from the given dataset path.
        Each inherited class must implement this method according to how their problem suite is structured.
        The implementation should call the _add_problem() method as many times as needed.
        """

        pass



    @abstractmethod
    def copy_problem_files_to_directory(
        self,
        problem: Problem,
        dir: str,
        *,
        include_tests: bool = False,
        include_solution: bool = False
    ):
        """
        Copies all the files required for an agent to solve a specific problem into a given directory.
        Each inherited class must implement this method according to how their problem suite is structured.
        
        Args:
            problem: Problem to copy files for
            dir: Directory to copy files to
            include_tests: Whether to include test files (default=False)
            include_solution: Whether to include solution files (default=False)
        """

        pass







    def run_agent_in_sandbox_for_problem(self, sandbox_manager, run_id, problem_name, agent_source_code, on_finish, *, timeout=None, include_solution=False):
        """
        Run an agent in a sandbox for the given problem.
        
        on_finish(result): Callback that receives a result when the sandbox finishes
            Success:
                result: {
                    "status": "success",
                    "diff": "..." | None,
                    "logs": <stdout+stderr> | None
                }
            Error:
                result: {
                    "status": "error",
                    "error": "..." | None,
                    "traceback": "..." | None,
                    "logs": <stdout+stderr> | None
                }
        """

        # Get problem data
        problem = self.get_problem(problem_name)
        if not problem:
            error(f"[PROBLEM_SUITE] Problem {problem_name} not found")
            raise ValueError(f"Problem {problem_name} not found")

        info(f"[PROBLEM_SUITE] Starting sandbox to run agent for problem {problem_name}")

        sandbox_id = None

        def on_mount(temp_dir):
            # Create /sandbox/repo directory
            repo_dir = os.path.join(temp_dir, "repo")
            os.makedirs(repo_dir, exist_ok=True)
            
            # Copy problem files to /sandbox/repo
            self.copy_problem_files_to_directory(problem_name, repo_dir, include_solution=include_solution)
            
            # Write agent source code to /sandbox/agent.py
            agent_path = os.path.join(temp_dir, "agent.py")
            with open(agent_path, "w") as f:
                f.write(agent_source_code)

        def _on_finish(result):
            if result.get("status") == "success":
                # Validate the diff before passing it on to the user
                diff = result.get("output")
                result["output"] = None
                result["diff"] = diff

                debug(f"[PROBLEM_SUITE] Validating diff generated by <{sandbox_id}> for {problem_name}")
                is_valid_diff, error_msg = validate_diff(diff, os.path.join(sandbox_manager.get_sandbox_temp_dir(sandbox_id), "repo"))
                if is_valid_diff:
                    debug(f"[PROBLEM_SUITE] Diff generated by <{sandbox_id}> for {problem_name} is valid")
                else:
                    warn(f"[PROBLEM_SUITE] Diff generated by <{sandbox_id}> for {problem_name} is invalid:\n{error_msg}")
                    result["status"] = "error"
                    result["error"] = f"Diff generated by <{sandbox_id}> for {problem_name} is invalid:\n{error_msg}"
            
            info(f"[PROBLEM_SUITE] Finished sandbox to run agent for problem {problem_name}: {result.get('status')}")

            # Call user's original callback
            on_finish(result)
        
        # Create sandbox that runs the AGENT_RUNNER.py script
        agent_runner_path = os.path.join(os.path.dirname(__file__), "AGENT_RUNNER.py")
        sandbox_id = sandbox_manager.create_sandbox(
            script_path=agent_runner_path,
            input_data={"problem_statement": problem.get("problem_statement")},
            env_vars={"RUN_ID": run_id},
            on_mount=on_mount,
            on_finish=_on_finish,
            timeout=timeout
        )

        debug(f"[PROBLEM_SUITE] Started sandbox to run agent for problem {problem_name}")



    def evaluate_solution_diff(self, sandbox_manager, run_id, problem_name, solution_diff, on_finish, *, timeout=None):
        """
        Evaluate a solution diff for the given problem.

        on_finish(result): Callback that receives a result when the sandbox finishes
            Success:
                result: {
                    "status": "success",
                    "test_results": "See the documentation for get_test_runner_path()",
                    "logs": <stdout+stderr> | None
                }
            Error:
                result: {
                    "status": "error",
                    "error": "..." | None,
                    "traceback": "..." | None,
                    "logs": <stdout+stderr> | None
                }
        """

        # Get problem data
        problem = self.get_problem(problem_name)
        if not problem:
            error(f"[PROBLEM_SUITE] Problem {problem_name} not found")
            raise ValueError(f"Problem {problem_name} not found")

        info(f"[PROBLEM_SUITE] Starting sandbox to evaluate solution diff for problem {problem_name}")

        sandbox_id = None

        def on_mount(temp_dir):
            # Create /sandbox/repo directory
            repo_dir = os.path.join(temp_dir, "repo")
            os.makedirs(repo_dir, exist_ok=True)
            
            # Copy problem files to /sandbox/repo
            self.copy_problem_files_to_directory(problem_name, repo_dir, include_tests=True)
            
            # Apply the diff to /sandbox/repo
            debug(f"[PROBLEM_SUITE] Applying agent's solution diff to {repo_dir} for problem {problem_name}")
            success, error_msg = apply_diff(solution_diff, repo_dir)
            if not success:
                raise Exception(f"Failed to apply agent's solution diff: {error_msg}")
            debug(f"[PROBLEM_SUITE] Applied agent's solution diff to {repo_dir} for problem {problem_name}")
        
        def _on_finish(result):
            if result.get("status") == "success":
                test_results = result.get("output")
                result["output"] = None
                result["test_results"] = test_results
            
            info(f"[PROBLEM_SUITE] Finished sandbox to evaluate solution diff for problem {problem_name}: {result.get('status')}")

            on_finish(result)
        
        # Create sandbox with test runner
        sandbox_id = sandbox_manager.create_sandbox(
            script_path=self.get_test_runner_path(),
            input_data={"tests": problem.get("tests")},
            env_vars={"RUN_ID": run_id}, # TODO
            on_mount=on_mount,
            on_finish=_on_finish,
            timeout=timeout
        )

        debug(f"[PROBLEM_SUITE] Started sandbox to evaluate solution diff for problem {problem_name}")